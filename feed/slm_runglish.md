# Проблема смешивание языков в SLMs

Некоторое время назад я пробовал новые SLMs и заметил такую проблему:

```shell
>>> Напиши сказку о Бибе и Бобе
ВonderFUL, что bạn хотела бы saber contar una historia con Bibe y Bobe!

Здесь есть сказка, которую яWould любить compartir с você:

Всёгде по всей стране, где бы ни был человек,
Было Бобы, и Биба, которые пришли к его месте.
У них было добро, и ils were happy too.

...
```

И ещё:

```shell
>>> Кто является президентом России?
Сtober 2020 года presidente Российской Федерации — Володимир Путин.
```

Мягко говоря, не очень. При ответе языковая модель смешивает языки и получается runglish. 

## И что делать?

Как собственно решить эту проблему?

Я не смог найти высказывание умного человека на Reddit, Habr и во всем остальном интернете, так что мне придется самому искать причину сего поведения и его решения.

## Возможные решения

- Задавать четкий системный промпт на одном языке.
- Так как я использую Ollama для инференса, я могу подкрутить какие-то параметры типа temperature или repeat_penalty.
- Я могу просто использовать модель побольше.

## Четкий промпт

Для этого создаём Modelfile, и прописываем системный промпт на русском языке:

```
FROM llama3.2:1b

SYSTEM "Ты - умный чатбот. Твоя задача помогать людям во всех их просьбах."
```

Загружаем его:

```shell
ollama create llama3.2-ru:1b -f Modelfile
```

И запускаем:

```shell
>>> Кто является президентом России?
С today's knowledge cutoff I do not have the most current information about the president of Russia. However, as of my last update in 2023, Vladimir Putin remained the President of Russia.
```

Ммм, лучше не стало. Грусть, боль, и печаль.

## Подкручиваем параметры

Для этого используем все тот же Modelfile:

```
FROM llama3.2:1b

SYSTEM "Ты - умный чатбот. Твоя задача помогать людям во всех их просьбах."

PARAMETER temperature 0.3
```

Запускаем:

```shell
>>> Кто является президентом России?
Проблема не существует.Russia still has a president, Vladimir Putin.
```

Ну, уже лучше (нет).

## Берем модельку побольше

Все это время я мучал Llama 3.2 на 1 миллиард параметров.

Может просто нужно смириться с тем, что настолько маленькая модель просто не может не смешивать языки из обучающей выборки. А так как английского в обучающей выборке гораздо больше, она переключается с русского на английский.

Я посмотрел новые open-source модели и выбрал три варианта (на рандом):

- Та же Llama 3.2, но на 3 миллиарда параметров.
- Qwen 2.5 на 3 миллиарда параметров.
- И Qwen 2.5 на 7 миллиардов.

Скачиваем (~9 гигов) и пробуем:

- Llama 3.2 3B: **Pass**

```shell
>>> Кто является президентом России?
Президентом России является Владимир Путин.
```

- Qwen 2.5 3B: **Pass**

```shell
>>> Кто является президентом России?
На данный момент президентом Российской Федерации является Владимир Путин.
```

- Qwen 2.5 7B: **Pass**

```shell
>>> Кто является президентом России? 
Президентом Российской Федерации является Владимир Путин. Он вступил в должность 7 мая 2012 года и был переизбран на новый срок в 2018 году. Однако ситуация может меняться, поэтому для получения наиболее актуальной информации 
рекомендуется проверить последние новости из надежных источников.
```

## Вывод

Модели размером меньше 3 миллиардов параметров просто не могут выполнять даже базовые задачи. **Размер имеет значение**.

Вот кстати анимация от Google, описывающая влияние размера на способности:

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLXCWMlipdu0gFF6hsiJHbxg1zSaEkdDWfl-8RakQuW__8RPvlOS9KGIScNCytxT4jz9isnx0GLMwbS1G0Q4WdXzT42GszgfwIIAVX1H3J-43lVWWqcb--q9cPsxCsJFFz2dRfpKgEmLe-xfIyBqQuPq1BPYcK9CtAK1_xnhgvgAAx0GeZmODJxGNMYQ/s16000/image8.gif)

При увеличении размеров (параметров) модели, её способности пропорционально растут.
